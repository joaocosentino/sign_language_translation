{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06c0892c",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9cc4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[65944]: Class CaptureDelegate is implemented in both /Users/georgiantanaselea/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x15c7765a0) and /Users/georgiantanaselea/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x1372a0860). One of the two will be used. Which one is undefined.\n",
      "objc[65944]: Class CVWindow is implemented in both /Users/georgiantanaselea/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x15c7765f0) and /Users/georgiantanaselea/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x127feca68). One of the two will be used. Which one is undefined.\n",
      "objc[65944]: Class CVView is implemented in both /Users/georgiantanaselea/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x15c776618) and /Users/georgiantanaselea/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x127feca90). One of the two will be used. Which one is undefined.\n",
      "objc[65944]: Class CVSlider is implemented in both /Users/georgiantanaselea/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x15c776640) and /Users/georgiantanaselea/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x127fecab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Preprocessing\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import utils, preprocessing\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "\n",
    "\n",
    "#Learning rate & decay\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "#Callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "# Model Training\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import convert_to_tensor\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.applications import VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c51ec7",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcaca117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory with training data\n",
    "train_dir = 'raw_data/archive/asl_alphabet_train/asl_alphabet_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0296dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(directory):\n",
    "    \"\"\"get images local and resizes them returning resized images and lables\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    directory_list = sorted(os.listdir(directory))\n",
    "    for i in range(len(directory_list)):\n",
    "        print(f\"Getting images of {directory_list[i]}:\")\n",
    "        for image in os.listdir(directory + \"/\" + directory_list[i])[:10]:\n",
    "            img = cv2.imread(directory + \"/\" + directory_list[i] + \"/\" + image)\n",
    "            img = crop_image(img)\n",
    "            img = cv2.resize(img, (32, 32))\n",
    "            images.append(img)\n",
    "            labels.append(directory_list[i])\n",
    "            print(f\"Got images of {directory_list[i]}: {len(images)}\")\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1e8aef",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7498c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_square(x_max, x_min, y_max, y_min):\n",
    "    '''function used as a helper function in crop images to return a square image'''\n",
    "    x_diff = x_max - x_min\n",
    "    y_diff = y_max - y_min\n",
    "\n",
    "    if y_diff > x_diff:\n",
    "\n",
    "        length_diff =  y_diff - x_diff\n",
    "\n",
    "        half_length_diff_max = round(length_diff/2)\n",
    "        half_length_diff_min = length_diff-half_length_diff_max\n",
    "\n",
    "        x_max = half_length_diff_max + x_max\n",
    "        x_min = x_min - half_length_diff_min\n",
    "\n",
    "\n",
    "    elif x_diff > y_diff:\n",
    "        length_diff =  x_diff -  y_diff\n",
    "\n",
    "        half_length_diff_max = round(length_diff/2)\n",
    "        half_length_diff_min = length_diff-half_length_diff_max\n",
    "\n",
    "        y_max = half_length_diff_max + y_max\n",
    "        y_min = y_min - half_length_diff_min\n",
    "\n",
    "\n",
    "    return x_max, x_min, y_max, y_min\n",
    "\n",
    "def crop_image(image):\n",
    "    '''a function to crop an image to just show their hands'''\n",
    "\n",
    "    mp_hands = mp.solutions.hands\n",
    "\n",
    "    mp_model = mp_hands.Hands(\n",
    "        static_image_mode=True, # only static images\n",
    "        max_num_hands=1, # max 2 hands detection\n",
    "        min_detection_confidence=0.4) # detection confidence\n",
    "\n",
    "    results = mp_model.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    h, w, c = image.shape # get image shape\n",
    "\n",
    "    hand_landmarks = results.multi_hand_landmarks\n",
    "        \n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        image_height, image_width, _ = image.shape\n",
    "        landmark_x = [landmark.x * image_width for landmark in hand_landmarks.landmark]\n",
    "        landmark_y = [landmark.y * image_height for landmark in hand_landmarks.landmark]\n",
    "        bbox = [min(landmark_x), min(landmark_y), max(landmark_x), max(landmark_y)]\n",
    "        crop_img = image[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n",
    "    else:\n",
    "        crop_img = image\n",
    "    pass\n",
    "\n",
    "    return crop_img\n",
    "\n",
    "\n",
    "def preprocessing(X, y):\n",
    "    \"\"\"Normalise our images and categorically encode our labels\"\"\"\n",
    "    LE = LabelEncoder()\n",
    "    X = np.array(X)\n",
    "    X = X/255\n",
    "    y = LE.fit_transform(y)\n",
    "    #y = utils.to_categorical(y, num_classes = 29)\n",
    "    return X, y\n",
    "\n",
    "def train_val_test_split(X, y):\n",
    "    \"\"\"Split the dataset into 6 different datasets to validate model performance\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.4)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size = 0.5)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e3d321e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images of A:\n",
      "Got images of A: 1\n",
      "Got images of A: 2\n",
      "Got images of A: 3\n",
      "Got images of A: 4\n",
      "Got images of A: 5\n",
      "Got images of A: 6\n",
      "Got images of A: 7\n",
      "Got images of A: 8\n",
      "Got images of A: 9\n",
      "Got images of A: 10\n",
      "Getting images of B:\n",
      "Got images of B: 11\n",
      "Got images of B: 12\n",
      "Got images of B: 13\n",
      "Got images of B: 14\n",
      "Got images of B: 15\n",
      "Got images of B: 16\n",
      "Got images of B: 17\n",
      "Got images of B: 18\n",
      "Got images of B: 19\n",
      "Got images of B: 20\n",
      "Getting images of C:\n",
      "Got images of C: 21\n",
      "Got images of C: 22\n",
      "Got images of C: 23\n",
      "Got images of C: 24\n",
      "Got images of C: 25\n",
      "Got images of C: 26\n",
      "Got images of C: 27\n",
      "Got images of C: 28\n",
      "Got images of C: 29\n",
      "Got images of C: 30\n",
      "Getting images of D:\n",
      "Got images of D: 31\n",
      "Got images of D: 32\n",
      "Got images of D: 33\n",
      "Got images of D: 34\n",
      "Got images of D: 35\n",
      "Got images of D: 36\n",
      "Got images of D: 37\n",
      "Got images of D: 38\n",
      "Got images of D: 39\n",
      "Got images of D: 40\n",
      "Getting images of E:\n",
      "Got images of E: 41\n",
      "Got images of E: 42\n",
      "Got images of E: 43\n",
      "Got images of E: 44\n",
      "Got images of E: 45\n",
      "Got images of E: 46\n",
      "Got images of E: 47\n",
      "Got images of E: 48\n",
      "Got images of E: 49\n",
      "Got images of E: 50\n",
      "Getting images of F:\n",
      "Got images of F: 51\n",
      "Got images of F: 52\n",
      "Got images of F: 53\n",
      "Got images of F: 54\n",
      "Got images of F: 55\n",
      "Got images of F: 56\n",
      "Got images of F: 57\n",
      "Got images of F: 58\n",
      "Got images of F: 59\n",
      "Got images of F: 60\n",
      "Getting images of G:\n",
      "Got images of G: 61\n",
      "Got images of G: 62\n",
      "Got images of G: 63\n",
      "Got images of G: 64\n",
      "Got images of G: 65\n",
      "Got images of G: 66\n",
      "Got images of G: 67\n",
      "Got images of G: 68\n",
      "Got images of G: 69\n",
      "Got images of G: 70\n",
      "Getting images of H:\n",
      "Got images of H: 71\n",
      "Got images of H: 72\n",
      "Got images of H: 73\n",
      "Got images of H: 74\n",
      "Got images of H: 75\n",
      "Got images of H: 76\n",
      "Got images of H: 77\n",
      "Got images of H: 78\n",
      "Got images of H: 79\n",
      "Got images of H: 80\n",
      "Getting images of I:\n",
      "Got images of I: 81\n",
      "Got images of I: 82\n",
      "Got images of I: 83\n",
      "Got images of I: 84\n",
      "Got images of I: 85\n",
      "Got images of I: 86\n",
      "Got images of I: 87\n",
      "Got images of I: 88\n",
      "Got images of I: 89\n",
      "Got images of I: 90\n",
      "Getting images of J:\n",
      "Got images of J: 91\n",
      "Got images of J: 92\n",
      "Got images of J: 93\n",
      "Got images of J: 94\n",
      "Got images of J: 95\n",
      "Got images of J: 96\n",
      "Got images of J: 97\n",
      "Got images of J: 98\n",
      "Got images of J: 99\n",
      "Got images of J: 100\n",
      "Getting images of K:\n",
      "Got images of K: 101\n",
      "Got images of K: 102\n",
      "Got images of K: 103\n",
      "Got images of K: 104\n",
      "Got images of K: 105\n",
      "Got images of K: 106\n",
      "Got images of K: 107\n",
      "Got images of K: 108\n",
      "Got images of K: 109\n",
      "Got images of K: 110\n",
      "Getting images of L:\n",
      "Got images of L: 111\n",
      "Got images of L: 112\n",
      "Got images of L: 113\n",
      "Got images of L: 114\n",
      "Got images of L: 115\n",
      "Got images of L: 116\n",
      "Got images of L: 117\n",
      "Got images of L: 118\n",
      "Got images of L: 119\n",
      "Got images of L: 120\n",
      "Getting images of M:\n",
      "Got images of M: 121\n",
      "Got images of M: 122\n",
      "Got images of M: 123\n",
      "Got images of M: 124\n",
      "Got images of M: 125\n",
      "Got images of M: 126\n",
      "Got images of M: 127\n",
      "Got images of M: 128\n",
      "Got images of M: 129\n",
      "Got images of M: 130\n",
      "Getting images of N:\n",
      "Got images of N: 131\n",
      "Got images of N: 132\n",
      "Got images of N: 133\n",
      "Got images of N: 134\n",
      "Got images of N: 135\n",
      "Got images of N: 136\n",
      "Got images of N: 137\n",
      "Got images of N: 138\n",
      "Got images of N: 139\n",
      "Got images of N: 140\n",
      "Getting images of O:\n",
      "Got images of O: 141\n",
      "Got images of O: 142\n",
      "Got images of O: 143\n",
      "Got images of O: 144\n",
      "Got images of O: 145\n",
      "Got images of O: 146\n",
      "Got images of O: 147\n",
      "Got images of O: 148\n",
      "Got images of O: 149\n",
      "Got images of O: 150\n",
      "Getting images of P:\n",
      "Got images of P: 151\n",
      "Got images of P: 152\n",
      "Got images of P: 153\n",
      "Got images of P: 154\n",
      "Got images of P: 155\n",
      "Got images of P: 156\n",
      "Got images of P: 157\n",
      "Got images of P: 158\n",
      "Got images of P: 159\n",
      "Got images of P: 160\n",
      "Getting images of Q:\n",
      "Got images of Q: 161\n",
      "Got images of Q: 162\n",
      "Got images of Q: 163\n",
      "Got images of Q: 164\n",
      "Got images of Q: 165\n",
      "Got images of Q: 166\n",
      "Got images of Q: 167\n",
      "Got images of Q: 168\n",
      "Got images of Q: 169\n",
      "Got images of Q: 170\n",
      "Getting images of R:\n",
      "Got images of R: 171\n",
      "Got images of R: 172\n",
      "Got images of R: 173\n",
      "Got images of R: 174\n",
      "Got images of R: 175\n",
      "Got images of R: 176\n",
      "Got images of R: 177\n",
      "Got images of R: 178\n",
      "Got images of R: 179\n",
      "Got images of R: 180\n",
      "Getting images of S:\n",
      "Got images of S: 181\n",
      "Got images of S: 182\n",
      "Got images of S: 183\n",
      "Got images of S: 184\n",
      "Got images of S: 185\n",
      "Got images of S: 186\n",
      "Got images of S: 187\n",
      "Got images of S: 188\n",
      "Got images of S: 189\n",
      "Got images of S: 190\n",
      "Getting images of T:\n",
      "Got images of T: 191\n",
      "Got images of T: 192\n",
      "Got images of T: 193\n",
      "Got images of T: 194\n",
      "Got images of T: 195\n",
      "Got images of T: 196\n",
      "Got images of T: 197\n",
      "Got images of T: 198\n",
      "Got images of T: 199\n",
      "Got images of T: 200\n",
      "Getting images of U:\n",
      "Got images of U: 201\n",
      "Got images of U: 202\n",
      "Got images of U: 203\n",
      "Got images of U: 204\n",
      "Got images of U: 205\n",
      "Got images of U: 206\n",
      "Got images of U: 207\n",
      "Got images of U: 208\n",
      "Got images of U: 209\n",
      "Got images of U: 210\n",
      "Getting images of V:\n",
      "Got images of V: 211\n",
      "Got images of V: 212\n",
      "Got images of V: 213\n",
      "Got images of V: 214\n",
      "Got images of V: 215\n",
      "Got images of V: 216\n",
      "Got images of V: 217\n",
      "Got images of V: 218\n",
      "Got images of V: 219\n",
      "Got images of V: 220\n",
      "Getting images of W:\n",
      "Got images of W: 221\n",
      "Got images of W: 222\n",
      "Got images of W: 223\n",
      "Got images of W: 224\n",
      "Got images of W: 225\n",
      "Got images of W: 226\n",
      "Got images of W: 227\n",
      "Got images of W: 228\n",
      "Got images of W: 229\n",
      "Got images of W: 230\n",
      "Getting images of X:\n",
      "Got images of X: 231\n",
      "Got images of X: 232\n",
      "Got images of X: 233\n",
      "Got images of X: 234\n",
      "Got images of X: 235\n",
      "Got images of X: 236\n",
      "Got images of X: 237\n",
      "Got images of X: 238\n",
      "Got images of X: 239\n",
      "Got images of X: 240\n",
      "Getting images of Y:\n",
      "Got images of Y: 241\n",
      "Got images of Y: 242\n",
      "Got images of Y: 243\n",
      "Got images of Y: 244\n",
      "Got images of Y: 245\n",
      "Got images of Y: 246\n",
      "Got images of Y: 247\n",
      "Got images of Y: 248\n",
      "Got images of Y: 249\n",
      "Got images of Y: 250\n",
      "Getting images of Z:\n",
      "Got images of Z: 251\n",
      "Got images of Z: 252\n",
      "Got images of Z: 253\n",
      "Got images of Z: 254\n",
      "Got images of Z: 255\n",
      "Got images of Z: 256\n",
      "Got images of Z: 257\n",
      "Got images of Z: 258\n",
      "Got images of Z: 259\n",
      "Got images of Z: 260\n",
      "Getting images of del:\n",
      "Got images of del: 261\n",
      "Got images of del: 262\n",
      "Got images of del: 263\n",
      "Got images of del: 264\n",
      "Got images of del: 265\n",
      "Got images of del: 266\n",
      "Got images of del: 267\n",
      "Got images of del: 268\n",
      "Got images of del: 269\n",
      "Got images of del: 270\n",
      "Getting images of nothing:\n",
      "Got images of nothing: 271\n",
      "Got images of nothing: 272\n",
      "Got images of nothing: 273\n",
      "Got images of nothing: 274\n",
      "Got images of nothing: 275\n",
      "Got images of nothing: 276\n",
      "Got images of nothing: 277\n",
      "Got images of nothing: 278\n",
      "Got images of nothing: 279\n",
      "Got images of nothing: 280\n",
      "Getting images of space:\n",
      "Got images of space: 281\n",
      "Got images of space: 282\n",
      "Got images of space: 283\n",
      "Got images of space: 284\n",
      "Got images of space: 285\n",
      "Got images of space: 286\n",
      "Got images of space: 287\n",
      "Got images of space: 288\n",
      "Got images of space: 289\n",
      "Got images of space: 290\n"
     ]
    }
   ],
   "source": [
    "#Loading and resizing the data by calling the get_images function\n",
    "images, labels = get_images(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c755efa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xda5e8b8b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e4923fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Calling the train_val_test_split to get the required dataset splits to validate model performance\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train, X_val, X_test, y_train, y_val, y_test \u001b[38;5;241m=\u001b[39m train_val_test_split(images, labels)\n\u001b[1;32m      3\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m preprocessing(X_train, y_train)\n\u001b[1;32m      4\u001b[0m X_val, y_val \u001b[38;5;241m=\u001b[39m preprocessing(X_val, y_val)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "#Calling the train_val_test_split to get the required dataset splits to validate model performance\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(images, labels)\n",
    "X_train, y_train = preprocessing(X_train, y_train)\n",
    "X_val, y_val = preprocessing(X_val, y_val)\n",
    "X_test, y_test = preprocessing(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cacc4b",
   "metadata": {},
   "source": [
    "# Model - Transfer Learning VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b43a82",
   "metadata": {},
   "source": [
    "## Loading VGG19 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7b667fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transfer_model():\n",
    "    model = VGG19(include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=X_train.shape[1:])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8e1e14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_nontrainable_layers(model):\n",
    "    \n",
    "    model.trainable = False\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b0b13",
   "metadata": {},
   "source": [
    "## Adding Predictive Layer and Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e1fbe46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_last_layers(model):\n",
    "    '''Take a pre-trained model, set its parameters as non-trainable, and add additional trainable layers on top'''\n",
    "    model = set_nontrainable_layers(model)\n",
    "    flattening_layer = Flatten()\n",
    "    dense_layer_1 = layers.Dense(128, activation='relu')\n",
    "    dense_layer_2 = layers.Dense(64, activation='relu')\n",
    "    dense_layer_3 = layers.Dense(32, activation='relu')\n",
    "    prediction_layer = layers.Dense(29, activation='softmax')\n",
    "    model = Sequential([\n",
    "    model,\n",
    "    flattening_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    dense_layer_3,\n",
    "    prediction_layer\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ebfaed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up a learning rate exponential decay\n",
    "lr_schedule = ExponentialDecay(\n",
    "    0.001,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "370f61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_model():\n",
    "    model = load_transfer_model()\n",
    "    model = add_last_layers(model)\n",
    "    opt = Adam(learning_rate = lr_schedule)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c4eb18a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_transfer_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f8fe84a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 2, 2, 512)         20024384  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               262272    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 29)                957       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,297,949\n",
      "Trainable params: 273,565\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a19c8a",
   "metadata": {},
   "source": [
    "### Defining Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a14bcb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='max', verbose=1, patience = 5, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77242a5",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2a553f8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa915d365664d7a8048be01eba88c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 07:33:50.951186: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-02-27 07:33:51.288759: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-27 07:33:52.045615: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [121], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, validation_data \u001b[38;5;241m=\u001b[39m (X_val,y_val), epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, callbacks \u001b[38;5;241m=\u001b[39m [es, TqdmCallback(verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)], verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data = (X_val,y_val), epochs = 100, callbacks = [es, TqdmCallback(verbose = 0)], verbose = 0, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e9e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history):\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label = 'train_accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.plot(history.history['loss'], label = 'train_loss')\n",
    "    plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae4acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a77ce1",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacef11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf9bbf",
   "metadata": {},
   "source": [
    "### Evaluating the Model using WebCam Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b166e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "webcam_test_data='raw_data/webcam-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe18200",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_webcam_test, labels_webcam_test = get_images(webcam_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c8951",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_webcam, y_test_webcam = preprocessing(images_webcam_test, labels_webcam_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e48689",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_webcam, accuracy_webcam = model.evaluate(X_test_webcam, y_test_webcam)\n",
    "loss_webcam, accuracy_webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae7aad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
